{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMpa5HT09RDq4rGA4/+P8KX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Safarys/crop_yield_prediction_using_ML/blob/main/randomforest_scrath_cypuml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Ac6M4-fiNQfy",
        "outputId": "f28a1811-6f6c-448c-9e12-50d78f31fe05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8bd5f5682c5a>\u001b[0m in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mean Squared Error: {mse}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"R-squared (R²) Score: {r2}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 ):\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m     \"\"\"\n\u001b[0;32m--> 989\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    958\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             )\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# --- Decision Tree from Scratch ---\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=10, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._grow_tree(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "        if (depth >= self.max_depth or num_samples < self.min_samples_split or len(np.unique(y)) == 1):\n",
        "            return self._most_common_label(y)\n",
        "\n",
        "        # Randomly select a subset of features\n",
        "        feature_indices = np.random.choice(num_features, int(np.sqrt(num_features)), replace=False)\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold = self._best_split(X, y, feature_indices)\n",
        "\n",
        "        # Grow the children recursively\n",
        "        left_indices, right_indices = self._split(X[:, best_feature], best_threshold)\n",
        "        left = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n",
        "        right = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n",
        "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"left\": left, \"right\": right}\n",
        "\n",
        "    def _best_split(self, X, y, feature_indices):\n",
        "        best_gain = -1\n",
        "        split_idx, split_thresh = None, None\n",
        "        for feature_index in feature_indices:\n",
        "            X_column = X[:, feature_index]\n",
        "            thresholds = np.unique(X_column)\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(X_column, y, threshold)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feature_index\n",
        "                    split_thresh = threshold\n",
        "        return split_idx, split_thresh\n",
        "\n",
        "    def _information_gain(self, X_column, y, split_thresh):\n",
        "        parent_loss = self._variance(y)\n",
        "        left_indices, right_indices = self._split(X_column, split_thresh)\n",
        "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "            return 0\n",
        "        n, n_left, n_right = len(y), len(left_indices), len(right_indices)\n",
        "        child_loss = (n_left / n) * self._variance(y[left_indices]) + (n_right / n) * self._variance(y[right_indices])\n",
        "        return parent_loss - child_loss\n",
        "\n",
        "    def _variance(self, y):\n",
        "        return np.var(y)\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_indices = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_indices = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_indices, right_indices\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        return np.mean(y)\n",
        "\n",
        "    def _traverse_tree(self, x, tree):\n",
        "        if isinstance(tree, dict):\n",
        "            feature_val = x[tree['feature']]\n",
        "            if feature_val <= tree['threshold']:\n",
        "                return self._traverse_tree(x, tree['left'])\n",
        "            else:\n",
        "                return self._traverse_tree(x, tree['right'])\n",
        "        return tree\n",
        "\n",
        "# --- RandomForest from Scratch ---\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
        "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.mean(tree_preds, axis=0)\n",
        "\n",
        "    def _bootstrap_sample(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        return X[indices], y[indices]\n",
        "\n",
        "# --- Load Data ---\n",
        "df = pd.read_csv('/content/Crop_production.csv')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_clean = df.drop(columns=['Unnamed: 0', 'State_Name'])\n",
        "\n",
        "# Handle categorical 'Crop_Type' by converting to numerical labels\n",
        "df_clean['Crop_Type'] = pd.factorize(df_clean['Crop_Type'])[0]\n",
        "\n",
        "# Features and target for yield prediction\n",
        "X = df_clean[['Crop_Type', 'N', 'P', 'K', 'pH', 'rainfall', 'temperature', 'Area_in_hectares']].values\n",
        "y = df_clean['Yield_ton_per_hec'].values\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Train RandomForest from Scratch ---\n",
        "rf = RandomForest(n_trees=10, max_depth=10)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# --- Make Predictions ---\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# --- Calculate Mean Squared Error (MSE) and R-squared (R²) Score ---\n",
        "mse = np.mean((y_test - y_pred) ** 2)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared (R²) Score: {r2}\")\n",
        "\n",
        "# --- Calculate Mean Absolute Percentage Error (MAPE) ---\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape}%\")\n",
        "\n",
        "# --- Calculate Accuracy from MAPE ---\n",
        "accuracy = 100 - mape\n",
        "print(f\"Regression Accuracy: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# --- Decision Tree from Scratch ---\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=5):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._grow_tree(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "        if (depth >= self.max_depth or num_samples < self.min_samples_split or len(np.unique(y)) == 1):\n",
        "            return self._most_common_label(y)\n",
        "\n",
        "        # Randomly select a subset of features\n",
        "        feature_indices = np.random.choice(num_features, int(np.sqrt(num_features)), replace=False)\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold = self._best_split(X, y, feature_indices)\n",
        "\n",
        "        # Split data and grow children recursively\n",
        "        left_indices, right_indices = self._split(X[:, best_feature], best_threshold)\n",
        "        left = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n",
        "        right = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n",
        "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"left\": left, \"right\": right}\n",
        "\n",
        "    def _best_split(self, X, y, feature_indices):\n",
        "        best_gain = -1\n",
        "        split_idx, split_thresh = None, None\n",
        "        for feature_index in feature_indices:\n",
        "            X_column = X[:, feature_index]\n",
        "            thresholds = np.unique(X_column)\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(X_column, y, threshold)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feature_index\n",
        "                    split_thresh = threshold\n",
        "        return split_idx, split_thresh\n",
        "\n",
        "    def _information_gain(self, X_column, y, split_thresh):\n",
        "        parent_loss = self._variance(y)\n",
        "        left_indices, right_indices = self._split(X_column, split_thresh)\n",
        "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "            return 0\n",
        "        n, n_left, n_right = len(y), len(left_indices), len(right_indices)\n",
        "        child_loss = (n_left / n) * self._variance(y[left_indices]) + (n_right / n) * self._variance(y[right_indices])\n",
        "        return parent_loss - child_loss\n",
        "\n",
        "    def _variance(self, y):\n",
        "        return np.var(y)\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_indices = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_indices = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_indices, right_indices\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        return np.mean(y)\n",
        "\n",
        "    def _traverse_tree(self, x, tree):\n",
        "        if isinstance(tree, dict):\n",
        "            feature_val = x[tree['feature']]\n",
        "            if feature_val <= tree['threshold']:\n",
        "                return self._traverse_tree(x, tree['left'])\n",
        "            else:\n",
        "                return self._traverse_tree(x, tree['right'])\n",
        "        return tree\n",
        "\n",
        "# --- RandomForest from Scratch ---\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=5, max_depth=5, min_samples_split=5):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
        "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.mean(tree_preds, axis=0)\n",
        "\n",
        "    def _bootstrap_sample(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        return X[indices], y[indices]\n",
        "\n",
        "# --- Load Data ---\n",
        "df = pd.read_csv('/content/Crop_production.csv')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_clean = df.drop(columns=['Unnamed: 0', 'State_Name'])\n",
        "\n",
        "# Handle categorical 'Crop_Type' by converting to numerical labels\n",
        "df_clean['Crop_Type'] = pd.factorize(df_clean['Crop_Type'])[0]\n",
        "\n",
        "# Features and target for yield prediction\n",
        "X = df_clean[['Crop_Type', 'N', 'P', 'K', 'pH', 'rainfall', 'temperature', 'Area_in_hectares']].values\n",
        "y = df_clean['Yield_ton_per_hec'].values\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Train RandomForest from Scratch ---\n",
        "rf = RandomForest(n_trees=5, max_depth=5)  # Reduced n_trees and max_depth\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# --- Make Predictions ---\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# --- Calculate Metrics ---\n",
        "mse = np.mean((y_test - y_pred) ** 2)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# --- Print Results ---\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared (R²) Score: {r2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQIayBV4RwSo",
        "outputId": "4774aa30-b6a4-4179-bcad-1a7aa0378ba9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 486.91476630091853\n",
            "R-squared (R²) Score: -0.2871462071409441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# --- Decision Tree from Scratch ---\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=5):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._grow_tree(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "        if (depth >= self.max_depth or num_samples < self.min_samples_split or len(np.unique(y)) == 1):\n",
        "            return self._most_common_label(y)\n",
        "\n",
        "        feature_indices = np.random.choice(num_features, int(np.sqrt(num_features)), replace=False)\n",
        "        best_feature, best_threshold = self._best_split(X, y, feature_indices)\n",
        "        left_indices, right_indices = self._split(X[:, best_feature], best_threshold)\n",
        "        left = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n",
        "        right = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n",
        "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"left\": left, \"right\": right}\n",
        "\n",
        "    def _best_split(self, X, y, feature_indices):\n",
        "        best_gain = -1\n",
        "        split_idx, split_thresh = None, None\n",
        "        for feature_index in feature_indices:\n",
        "            X_column = X[:, feature_index]\n",
        "            thresholds = np.unique(X_column)\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(X_column, y, threshold)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feature_index\n",
        "                    split_thresh = threshold\n",
        "        return split_idx, split_thresh\n",
        "\n",
        "    def _information_gain(self, X_column, y, split_thresh):\n",
        "        parent_loss = self._variance(y)\n",
        "        left_indices, right_indices = self._split(X_column, split_thresh)\n",
        "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "            return 0\n",
        "        n, n_left, n_right = len(y), len(left_indices), len(right_indices)\n",
        "        child_loss = (n_left / n) * self._variance(y[left_indices]) + (n_right / n) * self._variance(y[right_indices])\n",
        "        return parent_loss - child_loss\n",
        "\n",
        "    def _variance(self, y):\n",
        "        return np.var(y)\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_indices = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_indices = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_indices, right_indices\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        return np.mean(y)\n",
        "\n",
        "    def _traverse_tree(self, x, tree):\n",
        "        if isinstance(tree, dict):\n",
        "            feature_val = x[tree['feature']]\n",
        "            if feature_val <= tree['threshold']:\n",
        "                return self._traverse_tree(x, tree['left'])\n",
        "            else:\n",
        "                return self._traverse_tree(x, tree['right'])\n",
        "        return tree\n",
        "\n",
        "# --- RandomForest from Scratch ---\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=5, max_depth=5, min_samples_split=5):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
        "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.mean(tree_preds, axis=0)\n",
        "\n",
        "    def _bootstrap_sample(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        return X[indices], y[indices]\n",
        "\n",
        "# --- Load Data ---\n",
        "df = pd.read_csv('/content/Crop_production.csv')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_clean = df.drop(columns=['Unnamed: 0', 'State_Name'])\n",
        "\n",
        "# Handle categorical 'Crop_Type' by converting to numerical labels\n",
        "df_clean['Crop_Type'] = pd.factorize(df_clean['Crop_Type'])[0]\n",
        "\n",
        "# Features and target for yield prediction\n",
        "X = df_clean[['Crop_Type', 'N', 'P', 'K', 'pH', 'rainfall', 'temperature', 'Area_in_hectares']].values\n",
        "y = df_clean['Yield_ton_per_hec'].values\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Train RandomForest from Scratch ---\n",
        "rf = RandomForest(n_trees=5, max_depth=5)  # Reduced n_trees and max_depth for simplicity\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# --- Make Predictions ---\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# --- Calculate Metrics ---\n",
        "mse = np.mean((y_test - y_pred) ** 2)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# --- Calculate MAPE ---\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "# --- Calculate MAPE-based Accuracy ---\n",
        "accuracy = 100 - mape\n",
        "\n",
        "# --- Print Results ---\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared (R²) Score: {r2}\")\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape}%\")\n",
        "print(f\"Regression Accuracy (based on MAPE): {accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_KhJlNXVz1j",
        "outputId": "734e97ed-fc9f-4ebf-afab-c5e601c1710e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 406.0644619613484\n",
            "R-squared (R²) Score: -0.07342058249526473\n",
            "Mean Absolute Percentage Error (MAPE): inf%\n",
            "Regression Accuracy (based on MAPE): -inf%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-fcecae64e330>:127: RuntimeWarning: divide by zero encountered in divide\n",
            "  mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Calculate MAPE with Handling for Zero y_test Values ---\n",
        "# Filter out zero actual values to avoid division by zero in MAPE calculation\n",
        "non_zero_indices = y_test != 0\n",
        "y_test_non_zero = y_test[non_zero_indices]\n",
        "y_pred_non_zero = y_pred[non_zero_indices]\n",
        "\n",
        "# If there are any non-zero values, calculate MAPE and accuracy\n",
        "if len(y_test_non_zero) > 0:\n",
        "    mape = np.mean(np.abs((y_test_non_zero - y_pred_non_zero) / y_test_non_zero)) * 100\n",
        "    accuracy = 100 - mape\n",
        "else:\n",
        "    mape = np.inf\n",
        "    accuracy = 0\n",
        "\n",
        "# --- Calculate Metrics ---\n",
        "mse = np.mean((y_test - y_pred) ** 2)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# --- Print Results ---\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared (R²) Score: {r2}\")\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape}%\")\n",
        "print(f\"Regression Accuracy (based on MAPE): {accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm2eCGEsWuul",
        "outputId": "66edcb2c-645b-41ab-cf36-70766ca562bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 406.0644619613484\n",
            "R-squared (R²) Score: -0.07342058249526473\n",
            "Mean Absolute Percentage Error (MAPE): 242.08420377529674%\n",
            "Regression Accuracy (based on MAPE): -142.08420377529674%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scratch code of Random Forest"
      ],
      "metadata": {
        "id": "UY_vejzhYfoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# --- Decision Tree from Scratch ---\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=5):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._grow_tree(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "        if (depth >= self.max_depth or num_samples < self.min_samples_split or len(np.unique(y)) == 1):\n",
        "            return self._most_common_label(y)\n",
        "\n",
        "        # Randomly select a subset of features\n",
        "        feature_indices = np.random.choice(num_features, int(np.sqrt(num_features)), replace=False)\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold = self._best_split(X, y, feature_indices)\n",
        "\n",
        "        # Split data and grow children recursively\n",
        "        left_indices, right_indices = self._split(X[:, best_feature], best_threshold)\n",
        "        left = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n",
        "        right = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n",
        "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"left\": left, \"right\": right}\n",
        "\n",
        "    def _best_split(self, X, y, feature_indices):\n",
        "        best_gain = -1\n",
        "        split_idx, split_thresh = None, None\n",
        "        for feature_index in feature_indices:\n",
        "            X_column = X[:, feature_index]\n",
        "            thresholds = np.unique(X_column)\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(X_column, y, threshold)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feature_index\n",
        "                    split_thresh = threshold\n",
        "        return split_idx, split_thresh\n",
        "\n",
        "    def _information_gain(self, X_column, y, split_thresh):\n",
        "        parent_loss = self._variance(y)\n",
        "        left_indices, right_indices = self._split(X_column, split_thresh)\n",
        "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "            return 0\n",
        "        n, n_left, n_right = len(y), len(left_indices), len(right_indices)\n",
        "        child_loss = (n_left / n) * self._variance(y[left_indices]) + (n_right / n) * self._variance(y[right_indices])\n",
        "        return parent_loss - child_loss\n",
        "\n",
        "    def _variance(self, y):\n",
        "        return np.var(y)\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_indices = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_indices = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_indices, right_indices\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        return np.mean(y)\n",
        "\n",
        "    def _traverse_tree(self, x, tree):\n",
        "        if isinstance(tree, dict):\n",
        "            feature_val = x[tree['feature']]\n",
        "            if feature_val <= tree['threshold']:\n",
        "                return self._traverse_tree(x, tree['left'])\n",
        "            else:\n",
        "                return self._traverse_tree(x, tree['right'])\n",
        "        return tree\n",
        "\n",
        "# --- RandomForest from Scratch ---\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=5, max_depth=5, min_samples_split=5):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
        "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.mean(tree_preds, axis=0)\n",
        "\n",
        "    def _bootstrap_sample(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        return X[indices], y[indices]\n",
        "\n",
        "# --- Load Data ---\n",
        "df = pd.read_csv('/content/Crop_production.csv')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_clean = df.drop(columns=['Unnamed: 0', 'State_Name'])\n",
        "\n",
        "# Handle categorical 'Crop_Type' by converting to numerical labels\n",
        "df_clean['Crop_Type'] = pd.factorize(df_clean['Crop_Type'])[0]\n",
        "\n",
        "# Features and target for yield prediction\n",
        "X = df_clean[['Crop_Type', 'N', 'P', 'K', 'pH', 'rainfall', 'temperature', 'Area_in_hectares']].values\n",
        "y = df_clean['Yield_ton_per_hec'].values\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Train RandomForest from Scratch ---\n",
        "rf = RandomForest(n_trees=5, max_depth=5)  # Reduced n_trees and max_depth\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# --- Make Predictions ---\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# --- Calculate Metrics ---\n",
        "mse = np.mean((y_test - y_pred) ** 2)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# --- Calculate MAE for Accuracy ---\n",
        "mae = np.mean(np.abs(y_test - y_pred))\n",
        "accuracy = 100 - mae\n",
        "\n",
        "# --- Print Results ---\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared (R²) Score: {r2}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Regression Accuracy (based on MAE): {accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi5IQgSdXpXj",
        "outputId": "89018397-ebc7-43d9-aad2-4d11f9b26b9c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 335.33259308177935\n",
            "R-squared (R²) Score: 0.1135572276951593\n",
            "Mean Absolute Error (MAE): 2.317430070537241\n",
            "Regression Accuracy (based on MAE): 97.68256992946276%\n"
          ]
        }
      ]
    }
  ]
}